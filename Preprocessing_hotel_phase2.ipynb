{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tkinter as tk\n",
    "import pandas as pd\n",
    "from tkinter import filedialog, ttk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model  import LinearRegression , Ridge ,ElasticNet ,Lasso\n",
    "from sklearn.preprocessing import LabelEncoder ,PolynomialFeatures ,MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import ast\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from joblib import dump, load\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle nulls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def handle_nulls_in_train_data(train_data):\n",
    "    fill_values = {}\n",
    "    for column in train_data.columns:\n",
    "        if train_data[column].dtype == 'object':\n",
    "            fill_value = train_data[column].mode()[0]\n",
    "            fill_values[column] = fill_value\n",
    "            train_data[column].fillna(fill_value, inplace=True)\n",
    "        else:\n",
    "            fill_value = train_data[column].median()\n",
    "            fill_values[column] = fill_value\n",
    "            train_data[column].fillna(fill_value, inplace=True)\n",
    "    return train_data, fill_values\n",
    "\n",
    "def handle_nulls_in_test_data(test_data, fill_values):\n",
    "    for column in test_data.columns:\n",
    "        if test_data[column].dtype == 'object':\n",
    "            test_data[column].fillna(fill_values[column], inplace=True)\n",
    "        else:\n",
    "            test_data[column].fillna(fill_values[column], inplace=True)\n",
    "    return test_data\n",
    "\n",
    "# Load the training dataset\n",
    "# train_data = pd.read_csv('train.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Functions "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_date(data, col):\n",
    "    data[col] = pd.to_datetime(data[col])\n",
    "    data['year'] = data[col].dt.year.astype(float)\n",
    "    data['month'] = data[col].dt.month.astype(float)\n",
    "    data['day'] = data[col].dt.day.astype(float)\n",
    "    data = data.drop(['Review_Date'],axis= 1 )\n",
    "    return data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Get Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address(data):\n",
    "    col =data['Hotel_Address']\n",
    "    x=data['Hotel_Address'].str[-14:]\n",
    "    data['Hotel_Address'] = data['Hotel_Address'].apply(lambda x: x.split()[-1])\n",
    "    data['Hotel_Address'] =data['Hotel_Address'].apply(lambda x: 'United Kingdom' if x == 'Kingdom' else x)# united states\n",
    "    data['Hotel_Address'] =data['Hotel_Address'].apply(lambda x: 'United States' if x == 'States' else x)\n",
    "    return data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daysNo(data):\n",
    "    pattern = r'\\d+'\n",
    "    data['days_since_review'] = data['days_since_review'].apply(lambda x: int(re.findall(pattern, x)[0]))\n",
    "    return data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(data):\n",
    "  data['Tags'] = [ast.literal_eval(row) for row in data['Tags']]\n",
    "\n",
    "  for index, row in data['Tags'].items():\n",
    "    for name in row:\n",
    "      if \"trip\" in name:\n",
    "        data.at[index, 'Trip'] = name\n",
    "      if (\"room\" in name.lower()) or (\"suite\" in name.lower()) or (\"guestroom\" in name.lower()) or (\"studio\" in name.lower()) or (\"king\" in name.lower()):\n",
    "        data.at[index, 'Room'] = name\n",
    "      if \"night\" in name:\n",
    "        data.at[index, 'Nights'] = name\n",
    "  data = data.drop('Tags', axis=1)\n",
    "  return data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_Encoder(data, label, encoders=None):\n",
    "    if encoders is None:\n",
    "        encoders = {}\n",
    "    \n",
    "    cols = ('Hotel_Name', 'Reviewer_Nationality', 'Room', 'Trip', 'Nights', 'Hotel_Address', 'Positive_Review', 'Negative_Review')\n",
    "\n",
    "    if label:\n",
    "        for column in cols:\n",
    "            if column in data.columns:\n",
    "                encoders[column] = LabelEncoder()\n",
    "                data[column] = encoders[column].fit_transform(data[column])\n",
    "                print(f\"Encoded column: {column}\")\n",
    "                print(encoders)\n",
    "            else:\n",
    "                print(f\"Column '{column}' not found in the dataset.\")\n",
    "    else:\n",
    "        for col in cols:\n",
    "            if col in data.columns:\n",
    "                if col in encoders:\n",
    "                    new_values = data[col].unique()\n",
    "                    unseen_values = np.setdiff1d(new_values.astype(str), encoders[col].classes_.astype(str))\n",
    "                    if len(unseen_values) > 0:\n",
    "                        encoders[col].classes_ = np.append(encoders[col].classes_, unseen_values)\n",
    "                    data[col] = encoders[col].transform(data[col])\n",
    "                else:\n",
    "                    print(f\"LabelEncoder not found for column '{col}'.\")\n",
    "            else:\n",
    "                print(f\"Column '{col}' not found in the test dataset.\")\n",
    "\n",
    "    return data, encoders\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_fit_transform(X_train):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    # Save scaler object to disk\n",
    "    dump(scaler, 'scaler.joblib')\n",
    "    \n",
    "    return X_train_scaled\n",
    "\n",
    "def scaler_transform(X_test):\n",
    "    # Load scaler object from disk\n",
    "    scaler = load('scaler.joblib')\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    \n",
    "    return X_test_scaled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n",
      "290141\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('hotel-classification-dataset.csv')\n",
    "train_data['Reviewer_Score'] = train_data['Reviewer_Score'].map({'Low_Reviewer_Score': 0,'Intermediate_Reviewer_Score': 1, 'High_Reviewer_Score': 2})\n",
    "\n",
    "def handle_duplicated(data):\n",
    "    print(data.duplicated().sum())\n",
    "    data = data.drop_duplicates()\n",
    "    print(len(data))\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = handle_duplicated(train_data)\n",
    "for i, col in enumerate(train_data.select_dtypes(include=['number']).columns):\n",
    "    # axs[i, 0].scatter(data.index, data[col])\n",
    "    # axs[i, 0].set_xlabel('Index')\n",
    "    # axs[i, 0].set_ylabel(col)\n",
    "\n",
    "    Q1 = train_data[col].quantile(0.25)\n",
    "    Q3 = train_data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = train_data[(train_data[col] < lower_bound) | (train_data[col] > upper_bound)]\n",
    "    train_data = train_data[(train_data[col] >= lower_bound) & (train_data[col] <= upper_bound)]\n",
    "\n",
    "\n",
    "\n",
    "X = train_data.iloc[:, :-1]\n",
    "Y = train_data['Reviewer_Score']\n",
    "\n",
    "#Data Spliting \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done date\n",
      "done nulls\n",
      "done add\n",
      "done day\n",
      "done tags\n",
      "handel nulls done\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 130850 entries, 73746 to 216498\n",
      "Data columns (total 20 columns):\n",
      " #   Column                                      Non-Null Count   Dtype  \n",
      "---  ------                                      --------------   -----  \n",
      " 0   Hotel_Address                               130850 non-null  object \n",
      " 1   Additional_Number_of_Scoring                130850 non-null  int64  \n",
      " 2   Average_Score                               130850 non-null  float64\n",
      " 3   Hotel_Name                                  130850 non-null  object \n",
      " 4   Reviewer_Nationality                        130850 non-null  object \n",
      " 5   Negative_Review                             130850 non-null  object \n",
      " 6   Review_Total_Negative_Word_Counts           130850 non-null  int64  \n",
      " 7   Total_Number_of_Reviews                     130850 non-null  int64  \n",
      " 8   Positive_Review                             130850 non-null  object \n",
      " 9   Review_Total_Positive_Word_Counts           130850 non-null  int64  \n",
      " 10  Total_Number_of_Reviews_Reviewer_Has_Given  130850 non-null  int64  \n",
      " 11  days_since_review                           130850 non-null  int64  \n",
      " 12  lat                                         130850 non-null  float64\n",
      " 13  lng                                         130850 non-null  float64\n",
      " 14  year                                        130850 non-null  float64\n",
      " 15  month                                       130850 non-null  float64\n",
      " 16  day                                         130850 non-null  float64\n",
      " 17  Trip                                        130850 non-null  object \n",
      " 18  Room                                        130850 non-null  object \n",
      " 19  Nights                                      130850 non-null  object \n",
      "dtypes: float64(6), int64(6), object(8)\n",
      "memory usage: 25.0+ MB\n",
      "None\n",
      "Encoded column: Hotel_Name\n",
      "{'Hotel_Name': LabelEncoder()}\n",
      "Encoded column: Reviewer_Nationality\n",
      "{'Hotel_Name': LabelEncoder(), 'Reviewer_Nationality': LabelEncoder()}\n",
      "Encoded column: Room\n",
      "{'Hotel_Name': LabelEncoder(), 'Reviewer_Nationality': LabelEncoder(), 'Room': LabelEncoder()}\n",
      "Encoded column: Trip\n",
      "{'Hotel_Name': LabelEncoder(), 'Reviewer_Nationality': LabelEncoder(), 'Room': LabelEncoder(), 'Trip': LabelEncoder()}\n",
      "Encoded column: Nights\n",
      "{'Hotel_Name': LabelEncoder(), 'Reviewer_Nationality': LabelEncoder(), 'Room': LabelEncoder(), 'Trip': LabelEncoder(), 'Nights': LabelEncoder()}\n",
      "Encoded column: Hotel_Address\n",
      "{'Hotel_Name': LabelEncoder(), 'Reviewer_Nationality': LabelEncoder(), 'Room': LabelEncoder(), 'Trip': LabelEncoder(), 'Nights': LabelEncoder(), 'Hotel_Address': LabelEncoder()}\n",
      "Encoded column: Positive_Review\n",
      "{'Hotel_Name': LabelEncoder(), 'Reviewer_Nationality': LabelEncoder(), 'Room': LabelEncoder(), 'Trip': LabelEncoder(), 'Nights': LabelEncoder(), 'Hotel_Address': LabelEncoder(), 'Positive_Review': LabelEncoder()}\n",
      "Encoded column: Negative_Review\n",
      "{'Hotel_Name': LabelEncoder(), 'Reviewer_Nationality': LabelEncoder(), 'Room': LabelEncoder(), 'Trip': LabelEncoder(), 'Nights': LabelEncoder(), 'Hotel_Address': LabelEncoder(), 'Positive_Review': LabelEncoder(), 'Negative_Review': LabelEncoder()}\n",
      "encoding done in train\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 130850 entries, 73746 to 216498\n",
      "Data columns (total 20 columns):\n",
      " #   Column                                      Non-Null Count   Dtype  \n",
      "---  ------                                      --------------   -----  \n",
      " 0   Hotel_Address                               130850 non-null  int32  \n",
      " 1   Additional_Number_of_Scoring                130850 non-null  int64  \n",
      " 2   Average_Score                               130850 non-null  float64\n",
      " 3   Hotel_Name                                  130850 non-null  int32  \n",
      " 4   Reviewer_Nationality                        130850 non-null  int32  \n",
      " 5   Negative_Review                             130850 non-null  int32  \n",
      " 6   Review_Total_Negative_Word_Counts           130850 non-null  int64  \n",
      " 7   Total_Number_of_Reviews                     130850 non-null  int64  \n",
      " 8   Positive_Review                             130850 non-null  int32  \n",
      " 9   Review_Total_Positive_Word_Counts           130850 non-null  int64  \n",
      " 10  Total_Number_of_Reviews_Reviewer_Has_Given  130850 non-null  int64  \n",
      " 11  days_since_review                           130850 non-null  int64  \n",
      " 12  lat                                         130850 non-null  float64\n",
      " 13  lng                                         130850 non-null  float64\n",
      " 14  year                                        130850 non-null  float64\n",
      " 15  month                                       130850 non-null  float64\n",
      " 16  day                                         130850 non-null  float64\n",
      " 17  Trip                                        130850 non-null  int32  \n",
      " 18  Room                                        130850 non-null  int32  \n",
      " 19  Nights                                      130850 non-null  int32  \n",
      "dtypes: float64(6), int32(8), int64(6)\n",
      "memory usage: 21.0 MB\n",
      "None\n",
      "done date\n",
      "done nulls\n",
      "done add\n",
      "done day\n",
      "done tags\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 32713 entries, 128469 to 167140\n",
      "Data columns (total 20 columns):\n",
      " #   Column                                      Non-Null Count  Dtype  \n",
      "---  ------                                      --------------  -----  \n",
      " 0   Hotel_Address                               32713 non-null  object \n",
      " 1   Additional_Number_of_Scoring                32713 non-null  int64  \n",
      " 2   Average_Score                               32713 non-null  float64\n",
      " 3   Hotel_Name                                  32713 non-null  object \n",
      " 4   Reviewer_Nationality                        32713 non-null  object \n",
      " 5   Negative_Review                             32713 non-null  object \n",
      " 6   Review_Total_Negative_Word_Counts           32713 non-null  int64  \n",
      " 7   Total_Number_of_Reviews                     32713 non-null  int64  \n",
      " 8   Positive_Review                             32713 non-null  object \n",
      " 9   Review_Total_Positive_Word_Counts           32713 non-null  int64  \n",
      " 10  Total_Number_of_Reviews_Reviewer_Has_Given  32713 non-null  int64  \n",
      " 11  days_since_review                           32713 non-null  int64  \n",
      " 12  lat                                         32713 non-null  float64\n",
      " 13  lng                                         32713 non-null  float64\n",
      " 14  year                                        32713 non-null  float64\n",
      " 15  month                                       32713 non-null  float64\n",
      " 16  day                                         32713 non-null  float64\n",
      " 17  Trip                                        32713 non-null  object \n",
      " 18  Room                                        32713 non-null  object \n",
      " 19  Nights                                      32713 non-null  object \n",
      "dtypes: float64(6), int64(6), object(8)\n",
      "memory usage: 6.2+ MB\n",
      "None\n",
      "done encoder in test\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 32713 entries, 128469 to 167140\n",
      "Data columns (total 20 columns):\n",
      " #   Column                                      Non-Null Count  Dtype  \n",
      "---  ------                                      --------------  -----  \n",
      " 0   Hotel_Address                               32713 non-null  int32  \n",
      " 1   Additional_Number_of_Scoring                32713 non-null  int64  \n",
      " 2   Average_Score                               32713 non-null  float64\n",
      " 3   Hotel_Name                                  32713 non-null  int32  \n",
      " 4   Reviewer_Nationality                        32713 non-null  int32  \n",
      " 5   Negative_Review                             32713 non-null  int32  \n",
      " 6   Review_Total_Negative_Word_Counts           32713 non-null  int64  \n",
      " 7   Total_Number_of_Reviews                     32713 non-null  int64  \n",
      " 8   Positive_Review                             32713 non-null  int32  \n",
      " 9   Review_Total_Positive_Word_Counts           32713 non-null  int64  \n",
      " 10  Total_Number_of_Reviews_Reviewer_Has_Given  32713 non-null  int64  \n",
      " 11  days_since_review                           32713 non-null  int64  \n",
      " 12  lat                                         32713 non-null  float64\n",
      " 13  lng                                         32713 non-null  float64\n",
      " 14  year                                        32713 non-null  float64\n",
      " 15  month                                       32713 non-null  float64\n",
      " 16  day                                         32713 non-null  float64\n",
      " 17  Trip                                        32713 non-null  int32  \n",
      " 18  Room                                        32713 non-null  int32  \n",
      " 19  Nights                                      32713 non-null  int32  \n",
      "dtypes: float64(6), int32(8), int64(6)\n",
      "memory usage: 5.3 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def pre(data,fillvalues,encoder,label,test):\n",
    "    if label:\n",
    "        data = change_date(data,'Review_Date')\n",
    "        print('done date')\n",
    "        print('done nulls')\n",
    "        data = get_address(data)\n",
    "        print('done add')\n",
    "        data = get_daysNo(data)\n",
    "        print('done day')\n",
    "        data = get_tags(data)\n",
    "        print('done tags')\n",
    "        data ,fill_values= handle_nulls_in_train_data(data)\n",
    "        print('handel nulls done')\n",
    "        print(data.info())\n",
    "        data , encoders = Feature_Encoder(data, label=True)\n",
    "        print('encoding done in train')\n",
    "        print(data.info())\n",
    "        data = scaler_fit_transform(data)\n",
    "\n",
    "        return data ,fill_values,encoders\n",
    "\n",
    "    else:\n",
    "        if (test):  \n",
    "            data['Reviewer_Score'] = data['Reviewer_Score'].map({'Low_Reviewer_Score': 0,'Intermediate_Reviewer_Score': 1, 'High_Reviewer_Score': 2})\n",
    "            y_test = data['Reviewer_Score'] \n",
    "            data = data.iloc[:, :-1]\n",
    "            data = change_date(data,'Review_Date')\n",
    "            print('done date')\n",
    "            print('done nulls')\n",
    "            data = get_address(data)\n",
    "            print('done add')\n",
    "            data = get_daysNo(data)\n",
    "            print('done day')\n",
    "            data = get_tags(data)\n",
    "            print('done tags')\n",
    "            data = handle_nulls_in_test_data(data, fillvalues)\n",
    "            print(data.info())\n",
    "            data ,_ = Feature_Encoder(data,label=False, encoders=encoder)\n",
    "            print('done encoder in test')\n",
    "            print(data.info())\n",
    "            return data ,y_test\n",
    "        else:\n",
    "            data = change_date(data,'Review_Date')\n",
    "            print('done date')\n",
    "            print('done nulls')\n",
    "            data = get_address(data)\n",
    "            print('done add')\n",
    "            data = get_daysNo(data)\n",
    "            print('done day')\n",
    "            data = get_tags(data)\n",
    "            print('done tags')\n",
    "            data = handle_nulls_in_test_data(data, fillvalues)\n",
    "            print(data.info())\n",
    "            data ,_ = Feature_Encoder(data,label=False, encoders=encoder)\n",
    "            print('done encoder in test')\n",
    "            print(data.info())\n",
    "            data = scaler_transform(data)\n",
    "            return data\n",
    "    \n",
    "\n",
    "# test = pd.read_csv('test.csv')\n",
    "\n",
    "X_train,fillvalues ,encoders = pre(X_train,0,0,1,0)\n",
    "X_test = pre(X_test,fillvalues,encoders,0,0)\n",
    "# test = pre(test,fillvalues,encoders,0,1)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Hotel_Address  Additional_Number_of_Scoring  Average_Score  \\\n",
      "0            1.000000                      0.474640       0.370370   \n",
      "1            1.000000                      0.192279       0.481481   \n",
      "2            0.666667                      0.630583       0.740741   \n",
      "3            0.666667                      0.141559       0.740741   \n",
      "4            0.000000                      0.137774       0.518519   \n",
      "...               ...                           ...            ...   \n",
      "130845       0.000000                      0.143073       0.703704   \n",
      "130846       0.000000                      0.323240       0.555556   \n",
      "130847       1.000000                      0.747918       0.481481   \n",
      "130848       1.000000                      0.351249       0.666667   \n",
      "130849       1.000000                      0.235428       0.518519   \n",
      "\n",
      "        Hotel_Name  Reviewer_Nationality  Negative_Review  \\\n",
      "0         0.152495              0.437186         0.966845   \n",
      "1         0.981516              0.949749         0.392915   \n",
      "2         0.394640              0.949749         0.125444   \n",
      "3         0.480591              0.417085         0.378711   \n",
      "4         0.691312              0.065327         0.929273   \n",
      "...            ...                   ...              ...   \n",
      "130845    0.533272              0.417085         0.335404   \n",
      "130846    0.760628              0.231156         0.395954   \n",
      "130847    0.735675              0.517588         0.257721   \n",
      "130848    0.870610              0.869347         1.000000   \n",
      "130849    0.317006              0.351759         0.124114   \n",
      "\n",
      "        Review_Total_Negative_Word_Counts  Total_Number_of_Reviews  \\\n",
      "0                                0.315789                 0.373559   \n",
      "1                                0.035088                 0.135002   \n",
      "2                                0.245614                 0.764570   \n",
      "3                                0.035088                 0.274119   \n",
      "4                                0.280702                 0.181264   \n",
      "...                                   ...                      ...   \n",
      "130845                           1.000000                 0.200033   \n",
      "130846                           0.157895                 0.447975   \n",
      "130847                           0.228070                 0.760125   \n",
      "130848                           0.000000                 0.411590   \n",
      "130849                           0.438596                 0.176325   \n",
      "\n",
      "        Positive_Review  Review_Total_Positive_Word_Counts  \\\n",
      "0              0.941755                           0.148936   \n",
      "1              0.340235                           0.531915   \n",
      "2              0.407272                           0.319149   \n",
      "3              0.047317                           0.340426   \n",
      "4              0.082721                           0.127660   \n",
      "...                 ...                                ...   \n",
      "130845         0.632302                           0.297872   \n",
      "130846         0.819110                           0.148936   \n",
      "130847         0.093054                           0.425532   \n",
      "130848         0.239097                           0.404255   \n",
      "130849         0.163051                           0.085106   \n",
      "\n",
      "        Total_Number_of_Reviews_Reviewer_Has_Given  days_since_review  \\\n",
      "0                                             0.85           0.923288   \n",
      "1                                             0.05           0.769863   \n",
      "2                                             0.25           0.052055   \n",
      "3                                             0.25           0.068493   \n",
      "4                                             0.25           0.850685   \n",
      "...                                            ...                ...   \n",
      "130845                                        0.00           0.579452   \n",
      "130846                                        0.25           0.295890   \n",
      "130847                                        0.75           0.143836   \n",
      "130848                                        0.00           0.789041   \n",
      "130849                                        0.80           0.536986   \n",
      "\n",
      "             lat       lng  year     month       day  Trip      Room    Nights  \n",
      "0       0.874497  0.026608   0.0  0.727273  0.933333   1.0  0.811950  0.392857  \n",
      "1       0.872264  0.024757   0.5  0.000000  0.600000   1.0  0.108805  0.392857  \n",
      "2       0.995743  0.546225   1.0  0.454545  0.833333   1.0  0.147170  0.392857  \n",
      "3       0.992096  0.550026   1.0  0.454545  0.433333   1.0  0.231447  0.750000  \n",
      "4       0.493591  0.277127   0.0  0.909091  0.666667   0.0  0.071698  0.750000  \n",
      "...          ...       ...   ...       ...       ...   ...       ...       ...  \n",
      "130845  0.492200  0.276146   0.5  0.454545  0.166667   1.0  0.179874  0.750000  \n",
      "130846  0.490888  0.276559   0.5  1.000000  0.966667   1.0  0.082390  0.857143  \n",
      "130847  0.869823  0.018490   1.0  0.272727  0.633333   1.0  0.811950  0.000000  \n",
      "130848  0.872466  0.030542   0.5  0.000000  0.133333   1.0  0.534591  0.000000  \n",
      "130849  0.870546  0.027368   0.5  0.545455  0.200000   0.0  0.232075  0.392857  \n",
      "\n",
      "[130850 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.drop(['lat','Total_Number_of_Reviews_Reviewer_Has_Given','lng','year','day','Reviewer_Nationality'],axis = 1)\n",
    "# X_test = X_test.drop(['lat','Total_Number_of_Reviews_Reviewer_Has_Given','lng','year','day','Reviewer_Nationality'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # \n",
    "# svm = SVC(kernel='linear', C=0.1)\n",
    "# svm.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = svm.predict(X_test)\n",
    "\n",
    "# print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models regression  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def Logistic_Regression_model(X_train,Y_train,X_test,Y_test):\n",
    "#     start = time.time()\n",
    "#     model = LogisticRegression(C=1, max_iter=1000).fit(X_train, Y_train)\n",
    "#     with open('logistic_regression_model.pkl', 'wb') as file:\n",
    "#         pickle.dump(model, file)\n",
    "#     end = time.time()\n",
    "#     training_time = end - start\n",
    "#     start1 = time.time()\n",
    "#     with open('logistic_regression_model.pkl', 'rb') as file:\n",
    "#         lr_loaded = pickle.load(file)\n",
    "#     y_pred = lr_loaded.predict(X_test)\n",
    "#     accuracy = accuracy_score(Y_test, y_pred)\n",
    "#     end1 = time.time()\n",
    "#     test_time = end1 - start1\n",
    "#     # print(\"Accuracy = \", accuracy * 100, '%')\n",
    "#     # print('Training Time = ', training_time, 's')\n",
    "#     # print('Test Time = ', test_time, 's')\n",
    "\n",
    "#     return accuracy, training_time, test_time\n",
    "\n",
    "\n",
    "# def Decision_Tree_model(X_train, Y_train, X_test, Y_test):\n",
    "#     start_time = time.time()\n",
    "#     rf_classifier = DecisionTreeClassifier(max_depth=10)\n",
    "#     Y_train = np.ravel(Y_train)\n",
    "#     rf_classifier.fit(X_train, Y_train)\n",
    "#     with open('decision_tree_model.pkl', 'wb') as file:\n",
    "#         pickle.dump(rf_classifier, file)\n",
    "#     end_time = time.time()\n",
    "#     training_time = end_time - start_time\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     with open('decision_tree_model.pkl', 'rb') as file:\n",
    "#         dt_loaded = pickle.load(file)\n",
    "#     y_pred = dt_loaded.predict(X_test)\n",
    "#     end_time = time.time()\n",
    "#     test_time = end_time - start_time\n",
    "#     accuracy = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "#     # print(\"Accuracy = \", accuracy * 100, '%')\n",
    "#     # print('Training Time = ', training_time, 's')\n",
    "#     # print('Test Time = ', test_time, 's')\n",
    "\n",
    "#     return accuracy, training_time, test_time\n",
    "\n",
    "# def Random_Forest_model(X_train,Y_train,X_test,Y_test):\n",
    "#     start_time = time.time()\n",
    "#     rf_classifier = RandomForestClassifier(max_depth=10)\n",
    "#     Y_train = np.ravel(Y_train)\n",
    "#     rf_classifier.fit(X_train, Y_train)\n",
    "#     with open('random_forest_model.pkl', 'wb') as file:\n",
    "#         pickle.dump(rf_classifier, file)\n",
    "#     end_time = time.time()\n",
    "#     training_time = end_time - start_time\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     with open('random_forest_model.pkl', 'rb') as file:\n",
    "#         rf_loaded = pickle.load(file)\n",
    "#     y_pred = rf_loaded.predict(X_test)\n",
    "#     end_time = time.time()\n",
    "#     test_time = end_time - start_time\n",
    "#     accuracy = accuracy_score(Y_test, y_pred)\n",
    "#     # filename = 'finalized_model.sav'\n",
    "#     # pickle.dump(model, open(filename, 'wb'))\n",
    "#     # print(\"Accuracy = \", accuracy*100,'%')\n",
    "#     # print('Training Time = ',training_time,'s')\n",
    "#     # print('Test Time = ', test_time,'s')\n",
    "\n",
    "#     return accuracy, training_time, test_time\n",
    "\n",
    "# def Gradient_Boosting_model(X_train,Y_train,X_test,Y_test):\n",
    "#     start_time = time.time()\n",
    "#     # Create an instance of the GradientBoostingClassifier\n",
    "#     gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "#     # Fit the model to the training data\n",
    "#     Y_train = np.ravel(Y_train)\n",
    "#     gb_classifier.fit(X_train, Y_train)\n",
    "#     with open('Gradient_Boosting_model.pkl', 'wb') as file:\n",
    "#         pickle.dump(gb_classifier, file)\n",
    "#     end_time = time.time()\n",
    "#     training_time = end_time - start_time\n",
    "\n",
    "#     # Predict on the test data\n",
    "#     start_time = time.time()\n",
    "#     with open('Gradient_Boosting_model.pkl', 'rb') as file:\n",
    "#         gb_loaded = pickle.load(file)\n",
    "#     y_pred = gb_loaded.predict(X_test)\n",
    "#     end_time = time.time()\n",
    "#     test_time = end_time - start_time\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     accuracy = accuracy_score(Y_test, y_pred)\n",
    "#     # print(\"Accuracy = \", accuracy * 100,'%')\n",
    "#     # print('Training Time = ', training_time, 's')\n",
    "#     # print('Test Time = ', test_time, 's')\n",
    "#     return accuracy, training_time, test_time\n",
    "\n",
    "# def xgb_model(X_train,Y_train,X_test,Y_test):\n",
    "#     start_time = time.time()\n",
    "#     # Create an instance of the GradientBoostingClassifier\n",
    "#     gb_classifier = xgb.XGBClassifier( max_depth=3, learning_rate=0.1, n_estimators=100)\n",
    "\n",
    "#     # Fit the model to the training data\n",
    "#     Y_train = np.ravel(Y_train)\n",
    "#     gb_classifier.fit(X_train, Y_train)\n",
    "#     with open('xgb_model.pkl', 'wb') as file:\n",
    "#         pickle.dump(gb_classifier, file)\n",
    "#     end_time = time.time()\n",
    "#     training_time = end_time - start_time\n",
    "\n",
    "#     # Predict on the test data\n",
    "#     start_time = time.time()\n",
    "#     with open('xgb_model.pkl', 'rb') as file:\n",
    "#         gb_loaded = pickle.load(file)\n",
    "#     y_pred = gb_loaded.predict(X_test)\n",
    "#     end_time = time.time()\n",
    "#     test_time = end_time - start_time\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     accuracy = accuracy_score(Y_test, y_pred)\n",
    "#     # print(\"Accuracy = \", accuracy * 100,'%')\n",
    "#     # print('Training Time = ', training_time, 's')\n",
    "#     # print('Test Time = ', test_time, 's')\n",
    "#     return accuracy, training_time, test_time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calling models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# accuracy_rf, training_time_rf, test_time_rf = Random_Forest_model(X_train,y_train,X_test,y_test)\n",
    "# accuracy_gb, training_time_gb, test_time_gb = Gradient_Boosting_model(X_train,y_train,X_test,y_test)\n",
    "# accuracy_lr, training_time_lr, test_time_lr = Logistic_Regression_model(X_train,y_train,X_test,y_test)\n",
    "# accuracy_dt, training_time_dt, test_time_dt = Decision_Tree_model(X_train,y_train,X_test,y_test)\n",
    "# accuracy_xg, training_time_xg, test_time_xg = xgb_model(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ploting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=====================================\")\n",
    "# print(\"Logistic Regression Accuracy = {:.2%}\".format(accuracy_lr))\n",
    "# print(\"=====================================\")\n",
    "# print(\"Gradient Boosting Accuracy = {:.2%}\".format(accuracy_gb))\n",
    "# print(\"=====================================\")\n",
    "# print(\"Decision Tree Accuracy = {:.2%}\".format(accuracy_dt))\n",
    "# print(\"=====================================\")\n",
    "# print(\"XGB Accuracy = {:.2%}\".format(accuracy_xg))\n",
    "# print(\"=====================================\")\n",
    "# print(\"Random Forest Accuracy = {:.2%}\".format(accuracy_rf))\n",
    "# print(\"=====================================\")\n",
    "\n",
    "\n",
    "# # Print the Training time for each model\n",
    "# print(\"=====================================\")\n",
    "# print(f\"Logistic Regression Training Time:  (Time: {training_time_lr:.2f} seconds)\")\n",
    "# print(\"=====================================\")\n",
    "# print(f\"Gradient Boosting Training Time:  (Time: {training_time_gb:.2f} seconds)\")\n",
    "# print(\"=====================================\")\n",
    "# print(f\"Decision Tree Training Time:  (Time: {training_time_dt:.2f} seconds)\")\n",
    "# print(\"=====================================\")\n",
    "# print(f\"XGB Accuracy Training Time:  (Time: {training_time_xg:.2f} seconds)\")\n",
    "# print(\"=====================================\")\n",
    "# print(f\"Random Forest Training Time:  (Time: {training_time_rf:.2f} seconds)\")\n",
    "# print(\"=====================================\")\n",
    "# # Print the Test time for each model\n",
    "# print(\"=====================================\")\n",
    "# print(f\"Logistic Regression Testing Time:  (Time: {test_time_lr:.2f} seconds)\")\n",
    "# print(\"=====================================\")\n",
    "# print(f\"Gradient Boosting Testing Time:  (Time: {test_time_gb:.2f} seconds)\")\n",
    "# print(\"=====================================\")\n",
    "# print(f\"Decision Tree Testing Time:  (Time: {test_time_dt:.2f} seconds)\")\n",
    "# print(\"=====================================\")\n",
    "# print(f\"XGB Accuracy Testing Time:  (Time: {test_time_xg:.2f} seconds)\")\n",
    "# print(\"=====================================\")\n",
    "# print(f\"Random Forest Testing Time:  (Time: {test_time_rf:.2f} seconds)\")\n",
    "# print(\"=====================================\")\n",
    "# ##################\n",
    "# models = ['Logistic Regression', 'Gradient Boosting', 'Decision Tree', 'XGB Accuracy','Random Forest']\n",
    "# mse_values = [accuracy_lr,accuracy_gb,accuracy_dt,accuracy_xg,accuracy_rf]\n",
    "\n",
    "# # Set the width of the bars\n",
    "# bar_width = 0.35\n",
    "\n",
    "# # Create a numpy array for the x-axis positions of the bars\n",
    "# x_pos = range(len(models))\n",
    "\n",
    "# # Create the figure and axis objects\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# # Plot the bars\n",
    "# bars = ax.bar(x_pos, mse_values, bar_width)\n",
    "\n",
    "# # Add labels, title, and legend\n",
    "# ax.set_xlabel('Classification Models')\n",
    "# ax.set_ylabel('Accuracy')\n",
    "# ax.set_title('Comparison of Classification Models')\n",
    "# ax.set_xticks(x_pos)\n",
    "# ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "# ax.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n",
    "\n",
    "# n_groups = 5\n",
    "# means_frank = (training_time_lr, training_time_gb, training_time_dt, training_time_xg, training_time_rf)\n",
    "# means_guido = (test_time_lr, test_time_gb, test_time_dt, test_time_xg, test_time_rf)\n",
    "\n",
    "\n",
    "# # create plot\n",
    "# fig, ax = plt.subplots()\n",
    "# index = np.arange(n_groups)\n",
    "# bar_width = 0.2  # Reduce the bar width\n",
    "# opacity = 0.8\n",
    "\n",
    "# rects1 = ax.bar(index, means_frank, bar_width,\n",
    "# alpha=opacity,\n",
    "# color='b',\n",
    "# label='Training Time')\n",
    "\n",
    "# rects2 = ax.bar(index + bar_width, means_guido, bar_width,alpha=opacity,color='g',label='Testing Time')\n",
    "\n",
    "# ax.set_xlabel('Model')\n",
    "# ax.set_ylabel('Time (s)')\n",
    "# ax.set_title('Training and Testing Time by Model')\n",
    "# ax.set_xticks(index + bar_width / 2)\n",
    "# ax.set_xticklabels(('LogisticR', 'Gradient', 'Decision', 'XGB', 'Random'))\n",
    "# ax.legend()\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1083, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\losses.py\", line 2005, in categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 3) and (None, 5) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1312\\31536605.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Evaluate the model on the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 1083, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\losses.py\", line 2005, in categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis\n    File \"c:\\Users\\zeina\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 3) and (None, 5) are incompatible\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Convert the input values to one-hot encoded vectors\n",
    "X_train = keras.utils.to_categorical(X_train)\n",
    "\n",
    "# Convert the labels to one-hot encoded vectors\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=3)\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Conv1D(32, kernel_size=3, activation=\"relu\", input_shape=input_shape),\n",
    "  keras.layers.MaxPooling1D(pool_size=2),\n",
    "  keras.layers.Conv1D(64, kernel_size=3, activation=\"relu\"),\n",
    "  keras.layers.MaxPooling1D(pool_size=2),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(128, activation=\"relu\"),\n",
    "  keras.layers.Dropout(0.5),\n",
    "  keras.layers.Dense(5, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc =model.evaluate(keras.utils.to_categorical(X_test), keras.utils.to_categorical(y_test, num_classes=3))\n",
    "\n",
    "# Print the test accuracy\n",
    "print(\"Test accuracy:\", test_acc) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
